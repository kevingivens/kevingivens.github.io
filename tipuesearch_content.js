var tipuesearch = {"pages":[{"title":"About","text":"Hi! My name is Kevin Givens and welcome to my blog. I'm a quant living in New York City. Before working in finance, I earned a PhD in High Energy Physics. This blog is a place for me to study and share topics I find interesting. I hope you find them interesting as well. They will inevitably be a reflection of my interests in mathematics and computer science. About the title of the website, it's a reference to the Lyceum of Ancient Rome . This is the place where Aristotle founded the Peripetic school and lectured in philosophy. Like an aspiring member of the school, I'm trying to sort out ideas by walking around a bit. As usual, the views and opinions expressed here are my own and are not a reflection of my former and current employers.","tags":"pages","url":"/pages/about.html","loc":"/pages/about.html"},{"title":"Automating the Binding Process in Cython, Part 2","text":"Summary: We continue to discuss an approach towards automating the writing of Cython bindings. We focus on generating pyx files. Part 2, Overview Welcome back. In the previous post , I discussed generating pxd files (Cython C-level declarations) from C header files using pycparser . In this post, I want to explore using Cython's own parser to generate the corresponding Python wrapper classes and functions in a pyx files. As always, you can find the companion code to this post on my github page . Using Cython's Parser As a reminder from the last post, we are generating Python bindings for a C implementation of a trie data structure from the c-algorithms library (Incidentally, code from this same library is used as an example in the official Cython. documentation . There's a lot of overlap between that documentation of some of the topics discussed here). Cython's parser in written in Python. It's fairly straight-forward to use, though not terribly well documented. As with any conventional parser, each Cython declaration is represented as a node in an abstract syntax tree. The parser reads Cython code in pyx , pxd , or pxi files and generates C code that implements the CPython API. Our approach, borrowed from the autowrap project , is to use Cython's pxd reading capabilities to generate pyx files. After all, for most Cython projects, one tries to maintain some type of consistent standard when implementing wrapper functions and classes. Using a parser just implements these standards automatically. Compiler Pipeline Now for some code. Cython's pxd parser can be accessed programatically. In the snippet below, we parse a pxd file from the command line and return an AST. from Cython.Compiler.CmdLine import parse_command_line from Cython.Compiler.Main import create_default_resultobj , CompilationSource from Cython.Compiler import Pipeline from Cython.Compiler.Scanning import FileSourceDescriptor def parse_pxd_file ( path ): options , sources = parse_command_line ([ \"\" , path ]) path = os . path . abspath ( path ) basename = os . path . basename ( path ) name , ext = os . path . splitext ( basename ) source_desc = FileSourceDescriptor ( path , basename ) source = CompilationSource ( source_desc , name , os . getcwd ()) result = create_default_resultobj ( source , options ) context = options . create_context () pipeline = Pipeline . create_pyx_pipeline ( context , options , result ) context . setup_errors ( options , result ) # root of the AST/parse tree root = pipeline [ 0 ]( source ) We use the function parse_command_line to pass the source code located at path to the Cython compiler with no compiler flags turned on. We then create a pxy compiler Pipeline from a CompilationSource objects and a default options context . This pipeline is like a regular compiler pipeline through which phases of data transformation and optimizations occur. For our purposes, we will just walk the AST starting from the root node. Parsing our trie.pxd file from the previous post generates the following AST, which is schematically shown below (we leave off most leaves for presentation purposes): Pxd Visitor We implement a Pxd Visitor object following the protocol defined in Cython. In particular, our PxdVisitor implements a visit method for every type of node in the AST. For instance def visit_CStructOrUnionDefNode ( self , node ): # extract info from node return self . visitchildren ( node ) visits the CStructOrUnionDef type node in the AST representing a union or struct declarations. def visit_CTypeDefNode ( self , node ): # extract info from node return self . visitchildren ( node ) visits ctypedefs and so on. Our Visitor class walks all the nodes in the tree and collects information that we need to build the corresponding Python wrapper classes and functions. For a given struct (in our example the Trie struct), the visitor collects all the C functions that will become Python class methods. These are identified by the fact that their names match the name of the struct. For example, trie_new and trie_insert are functions for creating Trie structs and inserting data into them respectively. Once all the C functions are mapped to their corresponding structs, the Python classes can be built using string templates along with functions for managing type conversions between Python and C. We will explore this approach below. Python Class Lifetime Management The C structs defined in the pxd files should be wrapped by Python classes, as these are the closest language equivalent. Ideally, the Python classes will manage the lifetime of the corresponding C struct. This means that the C struct will be created when the Python class is created and it will be destroyed and its memory released when the Python class is destroyed. In this way, the C struct is \"buried under the hood\", so the speak, and the user of the Python class is essentially oblivious to its existence. For our trie example, the Python wrapper class looks like the following: cimport _trie cdef class Trie : cdef _trie . Trie * _this_ptr def __cinit__ ( self ): self . _this_ptr = _trie . trie_new () if self . _this_ptr is NULL : raise MemoryError () def __dealloc__ ( self ): if self . _this_ptr is not NULL : _trie . trie_free ( self . _this_ptr ) This ensures that the lifetime of the underlying C Trie struct is tied to the lifetime of the Python Trie class. Wrapping Functions Wrapping C functions is simple in principle. The idea is to cast the Python objects from the function signature into their nearest C equivalent type, then call the underlying C function via the classes' internal pointer and finally convert any returned C objects back to Python types. Wrapping functions is essentially an exercise in managing type conversions between C and Python. However, in practice, this can be a difficult task for a compiler to achieve. For instance, in our Trie example the insert method has the following C signature int trie_insert ( Trie * trie , char * key , TrieValue value ); The Trie *trie can be replaced with self.this_ptr . The char* key argument can be replaced with a Python string (more on this below). TrieValue is a typedef of void* , which is C approach to generic programming. From the Python side, we have a few choices in terms preserving this generacy. One approach would be to declare value to be a generic Python type object and then attempt to cast it to a <void*> in the C function call, i.e. def insert ( self , ... , object value ); return < int > _trie . trie_insert ( self . this_ptr , ... , < void *> value ) However this could easily fail if the user passed in a nonsensical value object. The other approach, as advocated by the Cython documentation , is to specify a concrete type in the Python function call, for instance, int or double. This breaks type generacy but prevents runtime errors. Interestingly, this is also the approach used by autowrap to handle C++ templates. In autowrap, the user can specify the concrete Python type they wish to implement using a compiler directive. This eases the burdon of having to implement highly redundant Python classes for every concrete Python type one wishes to use. I may implement a compiler directive like this at some point in the future. As for the char * in the C function, exposing a Python str (unicode in Python 3) is thoroughly in the Cython documentation. We just implement the type conversion directly. So a naive Python wrapper would look like the following 1 def insert ( self , str key , int value ): py_byte_str = key . encode ( 'UTF-8' ) cdef char * c_key = py_byte_str return self . _this_ptr . trie_insert ( Trie * trie , c_key , < void *> value ) There is one more problem with this approach, namely that the returned int is not really meant to be an integer, per se. It's an int from a C function call indicating failure by a 0 and success by a positive value. This is one of C's (limited) approach to runtime error handling. Clearly, a parser just looking at the pxd return type cannot distinguish between an int of this type and a regular int. Cython provides an alternative type, bint , as in binary int, that can be used for these types of function calls. A bint auto-converts to a Python bool instead of an int . So for our pxd parser to pick it up, we would have to manually update our pxd file from int trie_insert ( Trie * trie , char * key , TrieValue value ); to bint trie_insert ( Trie * trie , char * key , TrieValue value ); The returned bint 's value should be checked and an exception should be raised if it is false. So am improved wrapper look like the following def insert ( self , str key , int value ): py_byte_str = key . encode ( 'UTF-8' ) cdef char * c_key = py_byte_str if not _trie . trie_insert ( self . _thisptr , c_key , < void *> value ): raise MemoryError () Python Protocols and Special Methods As an easier wrapping example, the length of the trie struct can be determined via the following function unsigned int trie_num_entries ( Trie * trie ); Our python wrapper is simply def num_enties ( self ): return _trie . trie_num_entries ( self . _this_ptr ) Clearly, Python users would expect a __len__() special method instead of num_entries() . We can either allow users to adjust the function name manually after generating the pyx file or directly map num_enties to __len__ in the pxd parser. We'll use a direct mapping for now but it's by no means a general solution. So our length method would look like the following def __len__ ( self ): return _trie . trie_num_entries ( self . _this_ptr ) This problem can emerge for any C functions that implements Python protocol functionality, such as __get__() , __set__() , __getitem__(key) , __setitem__(key, value) , etc. Handling Includes Includes should be one of the simple aspects of the parser. However, there is one subtlety that needs to be addressed. In particular, Cython has a convention whereby for any pyx file, say foo.pyx , all C declarations from a pxd file with the same name, e.g. foo.pxd , are automatically included at compile time. This can cause a name collision if we wish to give our Python classes and functions the same name in Python as they have in the underlying C library. One approach to avoiding name collisions is to first name the pxd file _foo.pxd (add a leading underscore) to prevent it from being automatically included in foo.pyx . Then rename the imported C declarations with a leading underscore in the pyx file. This will prevent C and Python names from colliding in the pyx file. Putting it All Together So, running our pyx generator and manually replacing void* and bint we get the following Python wrapper class cimport trie cdef class Trie : cdef trie . Trie * _this_ptr def __cinit__ ( self , ): self . _this_ptr = trie . trie_new () if self . _this_ptr is NULL : raise MemoryError () def __dealloc__ ( self ): if self . _this_ptr is not NULL : trie . trie_free ( self . _this_ptr ) cdef insert ( self , str key , int value ): py_byte_str = key . encode ( 'UTF-8' ) cdef char * c_key = py_byte_str if not trie . trie_insert ( self . _thisptr , c_key , < void *> value ): raise MemoryError () cdef insert_binary ( self , str key , int key_length , int value ): py_byte_str = key . encode ( 'UTF-8' ) cdef char * c_key = py_byte_str if not trie . trie_insert ( self . _thisptr , c_key , < void *> value ): raise MemoryError () cdef lookup ( self , str key ): py_byte_str = key . encode ( 'UTF-8' ) cdef char * c_key = py_byte_str return < int > trie . trie_lookup ( self . _this_ptr , c_key ) cdef lookup_binary ( self , str key , int key_length ): py_byte_str = key . encode ( 'UTF-8' ) cdef char * c_key = py_byte_str return < int > trie . trie_lookup_binary ( self . _this_ptr , c_key , < int > key_length ) cdef remove ( self , str key ): py_byte_str = key . encode ( 'UTF-8' ) cdef char * c_key = py_byte_str return < int > trie . trie_remove ( self . _this_ptr , c_key ) cdef remove_binary ( self , str key , int key_length ): py_byte_str = key . encode ( 'UTF-8' ) cdef char * c_key = py_byte_str return < int > trie . trie_remove_binary ( self . _this_ptr , c_key , < int > key_length ) cdef __len__ ( self ): return < int > trie . trie_num_entries ( self . _this_ptr ) Final Words There's much more work to do on the pxy generator. For instance, the following items still need to be handled Enums Compiler directives Comments More special function mapping Exception Handling In my next post, I will revisit this process for C++. In particular, I'll use libClang to parse C++ header files along with autowrap for Pyx generation. See you next time. See Cython's documentation on the need for the temporary py_byte_str object ↩","tags":"Cython","url":"/automating-the-binding-process-in-cython-part-2.html","loc":"/automating-the-binding-process-in-cython-part-2.html"},{"title":"Automating the Binding Process in Cython","text":"Summary: We discuss an approach towards automating the writing of Cython bindings Part 1, Overview Cython is a wonderful tool for writing Python bindings. It gives the developer a tremendous amount of control over both code performance and semantics in a language that is a superset of Python. It's no surprise much of the Scientific Python ecosystem uses Cython to wrap C and C++ libraries. However, in this post, I want to discuss one of the common pain points of Cython as well an approach I'm currently working on to alleviate some of this pain. The pain point I'm focusing on is the shear amount of code one needs to write in Cython. Consider the following dummy C++ class // foo.hpp class Foo { Foo ( int a ); int foo_meth ( int b ); } In order to build cython bindings for this class, one would need write, at a minimum, one pxd file for the c-level declaration # foo.pxd cdef cppclass Foo : Foo ( int a ) int foo_meth ( int b ) and one pyx file for the python class that wraps the C++ classes # foo.pyx cdef class Foo : cdef shared_ptr [ _foo . Foo ] _thisptr def __cinit__ ( self , a ): self . _thisptr = _foo . Foo (< int > a ) def __dealloc__ ( self ): if self . _thisptr == NULL : self . _thisptr . reset () def foo_meth ( b ): return self . _thisptr . foo_meth (< int > a ) So you can see that 3 lines from a C++ file becomes ~12 lines of Cython. This isn't a major problem for small C or C++ libraries, but for large libraries with 1000's of interface files, writing Cython bindings for the entire library becomes a colossal task. A second problem can arise when a C or C++ library is under heavy active development. Public interfaces may change significantly from one release to the next rendering your hard-fought Cython bindings obsolete. Like Sisyphus, you're plagued with the eternal task of keeping your bindings up to date with the latest release. One natural approach to resolving this dilemma to write a computer program that automatically generates these bindings for you, \"on the fly\", every time there is a new release of the underlying C or C++ library. It's this approach I want to discuss in here. The essence of the idea is familiar for anyone who has studied compilers. If you want to transform data (in this case, code) from one language (C/C+) to another (Cython) you need a parser. The work flow is to first translate the C/C++ source code into an abstract syntax tree and then to walk the tree and generate the transformed Cython code. So all we need in order to give this approach a try is a C/C++ parser. Know any good ones? Just kidding, sort of. Until recently, the only C/C++ parser in the open source world was buried inside GCC along with linkers, Fortran parsers, and various other goodies. Thankfully, this situation has improved with the LLVM project. Clang is production quality C family compiler whose parser is accessible in python through the libClang bindings. The story for C code, is even better. There is an open source C parser written in pure python called pycparser . We can actually take this parsing approach one step further. We can use the C/C++ parsers to translate source code into C-level declarations in pxd files and then use Cython's own parser to translate from pxd files into pyx files. This latter step is the approach taken by the autowrap project. A schematic overview of the process is given below Pycparser Pycparser is a C parser written in pure python. We can use it to parse C header files and generate corresponding pxd files. For the purpose of this post, we'll use an example C file from the c-algorithms library In particular, we'll focus on its implementation of a trie data structure (trie.h), annotated in the snippet below: // trie.h typedef struct _Trie Trie ; typedef void * TrieValue ; Trie * trie_new ( void ); void trie_free ( Trie * trie ); int trie_insert ( Trie * trie , char * key , TrieValue value ); ... The idea is to use pycparser to translate this to the equivalent cython declarations in a pxd file. The generated pxd file would look like this: # trie.pxd cdef extern from \"c-algorithms/src/trie.h\" : ctypedef struct Trie : pass ctypedef void * TrieValue Trie * trie_new () void trie_free ( Trie * trie ) int trie_insert ( Trie * trie , char * key , TrieValue value ) ... So the translation consists of the following: placing all statements inside a cdef extern block typedef goes to ctypedef trailing ; is removed from each statement typedef struct goes to pass (skipped implementation) trie_new(void) has void removed AST These translation are implemented by walking a intermediate data representation known as an abstract syntax tree (AST). Each node in the tree is \"visited\" by a corresponding visit function. The AST for this example looks like the following: In order to walk the AST, I wrote a Cython generator class. This is slight modification of the C generator that comes with the pycparser release. Take a look at my github for more details. The idea is to provide a visit_foo method for each node type that generates the correct Cython implemenation code. Some highlights from the generator are given below def visit_Typedef ( self , n ): s = 'c' # prepend 'typedef' with 'c' if n . storage : s += ' ' . join ( n . storage ) + ' ' # handle typedef struct definition if type ( n . type ) == c_ast . TypeDecl and type ( n . type . type ) == c_ast . Struct : s += self . _handle_typedef_struct ( n . type ) else : s += self . _generate_type ( n . type ) return s def visit_ParamList ( self , n ): ''' return '' if 'void' is only argument ''' param_list = ', ' . join ( self . visit ( param ) for param in n . params ) if param_list == 'void' : return '' else : return param_list In my next post, I will discuss how to use the Cython parser itself to parse pxd files and generate pyx files.","tags":"Cython","url":"/automating-the-binding-process-in-cython.html","loc":"/automating-the-binding-process-in-cython.html"},{"title":"Cython Bindings in PyQL","text":"Summary: We review the general approach to Cython bindings in the PyQL library. Recently, I've written a few posts, here and here , about my contributions to the PyQL library. I thought I would take the opportunity to review the general approach I've been using. Although Cython is a fantastic tool for writing Python bindings, its support for some of C++'s more advanced features (e.g. templates, smart pointers) is not well documented online. Hopefully these notes may be of use to Cython binding writers in general. The Basic Idea The basic approach to writing bindings is to build wrapper functions around each function in the compiled language use wish to export to users. These wrapper functions manage the input and output of data to the compiled functions, such that the user of the wrapper function can safely ignore the details of the compiled language. A schematic of this is given below 1 This approach applies to classes as well. Let's take a look at a non-trivial example from PyQL to better understand the approach. Consider the CreditDefaultSwap class from Quantlib(ql/instruments/creditdefaultswap.hpp). Its class definition looks like the following class CreditDefaultSwap : public Instrument { CreditDefaultSwap ( Protection :: Side side , Real notional , Rate spread , ... const DayCounter & lastPeriodDayCounter = DayCounter (), const bool rebatesAccrual = true ); ... Protection :: Side side () const ; Real notional () const ; ... Where I've ignored most of the constructor arguments and class methods to simplify the discussion. To expose this class in PyQL we first declare it in a cython definition file (.pxd) as follows cdef extern from 'ql/instruments/creditdefaultswap.hpp' namespace 'QuantLib' : cdef cppclass CreditDefaultSwap ( Instrument ): CreditDefaultSwap ( Side side , Real notional , Rate spread , ... DayCounter & last_period_day_counter , bool rebates_accrual ) int side () Real notional () ... For each of these declared classes we build a corresponding wrapper class in python. These python classes are defined in cython implementation file (.pyx) and are importable as python modules. For the credit default swap, the python wrapper class is given below cdef class CreditDefaultSwap ( Instrument ): def __init__ ( self , Side side , double notional , double spread , ... DayCounter last_period_day_counter = Actual360 ( True ), bool rebates_accrual = True ): \"\"\"Credit default swap as running-spread only \"\"\" self . _thisptr = shared_ptr [ _instrument . Instrument ]( new _cds . CreditDefaultSwap ( side , notional , spread , deref ( schedule . _thisptr ), payment_convention , deref ( day_counter . _thisptr ), settles_accrual , pays_at_default_time , deref ( protection_start . _thisptr ), shared_ptr [ _cds . Claim ](), deref ( last_period_day_counter . _thisptr ), rebates_accrual ) ) From this example we make the following observations: The constructor exposes Python objects to the user. Simple types like double and bool are automatically converted by the cython compiler to the corresponding python type. More complex types such as DayCounter are defined elsewhere in the PyQL and imported to this file. The wrapper class manages the C++ class by means of a internal smart pointer self._thisptr = shared_ptr[_instrument.Instrument](new _cds.CreditDefaultSwap ... This pointer controls the lifetime of the corresponding C++ object along with any methods that are applied to it. When the python object goes out of scope it resets the smart pointer which frees the memory that was allocated to the C++ object. Internal smart pointers are also used to pass C++ objects into the C++ constructor. For instance deref(schedule._thisptr) Inheritance is managed by casting the smart pointer from Instrument down to CreditDefaultSwap. Most classes in PyQL have utility functions to perform this cast. For the CDS, it's given below cdef inline _cds . CreditDefaultSwap * _get_cds ( CreditDefaultSwap cds ): return < _cds . CreditDefaultSwap *> cds . _thisptr . get () This allows us to access the CDS's methods like the following snippet @property def notional ( self ): return _get_cds ( self ) . notional () Import Dilemma The astute reader (you're all astute, I'm sure) will have noticed that both the C++ object and the Python object have the same name CreditDefaultSwap . This causes a name collision and a dilemma. What's the best way to avoid such a collision? Some libraries take the approach of renaming the C/C++ object Foo_C or the Python object Foo_Py. PyQL takes a different approach. They first place the C++ objects in an underscored pxd file. For instance `_credit_default_swap.pxd'. They then import these objects into the pyx files as follows import _credit_default_swap as _cds The reason for the the underscore is that definition files with the same name as a pyx file, credit_default_swap.pxd , are automatically imported into credit_default_swap.pyx This leads to the situation where there are three files for every one hpp file. This isn't ideal, but at least it's consistent and avoids having to rename C++ objects. Python Data Structures One of the main reasons to prefer Cython over other binding tools is its flexibility. PyQL takes advantage of this flexibility to make Quantlib more compatible with standard Python data structures. For example, Quantlib provides custom Date and Matrix objects that are used throughout the library. Python users would naturally prefer to use datetime.Date and NumPy arrays instead of the custom objects. PyQL provides utilities for converting between these standard Python objects. Numpy arrays are converted via quantlib.math.matrix.pyx def to_ndarray ( self ): cdef np . npy_intp [2] dims dims [ 0 ] = self . _thisptr . rows () dims [ 1 ] = self . _thisptr . columns () cdef arr = np . PyArray_SimpleNew ( 2 , & dims [ 0 ], np . NPY_DOUBLE ) cdef double [ :,:: 1 ] r = arr cdef size_t i , j for i in range ( dims [ 0 ]): for j in range ( dims [ 1 ]): r [ i , j ] = self . _thisptr [ i ][ j ] return arr Similarly, datetime.date's are converted via quantlib.time.date.pyx def object _pydate_from_qldate ( QlDate qdate ): \"\"\" Converts a QuantLib Date (C++) to a datetime.date object. \"\"\" cdef int m = qdate . month () cdef int d = qdate . dayOfMonth () cdef int y = qdate . year () return date_new ( y , m , d ) Credit: David Beazley Swig Master Class . ↩","tags":"Coding","url":"/cython-bindings-in-pyql.html","loc":"/cython-bindings-in-pyql.html"},{"title":"Parsing ITCH Messages in C++","text":"Summary: We review how to parse ITCH messages in C++ with configurable code The ITCH protocol is a message protocol used for communicating with the NASDAQ stock exchange. In this post, I wanted to review how to parse ITCH messages in C++. As always, you can find my sample code to go along with this post on my github page . The latest version of protocol is 5.0 and its spec can be found here . ITCH defines a series of messages along with the type and size of each of the message's components. For example, an Add order looks like this: Add Order Message Name Offset Length Value Notes Message Type 0 1 \"A\" Add Order – No MPID Attribution Message. Stock Locate 1 2 Integer Locate code identifying the security Tracking Number 3 2 Integer Nasdaq internal tracking number Timestamp 5 6 Integer Nanoseconds since midnight. Order Reference Number 11 8 Integer Unique reference number assigned to new order at the time of receipt. Buy/Sell Indicator 19 1 Alpha The type of order being added. \"B\" = Buy \"S\" = Sell Shares 20 4 Integer The total number of shares associated with the order being added to the book. Stock 24 8 Alpha Stock symbol, right padded with spaces Price 32 4 Price The display price of the new order An example add order might look like this const char msg [] = { 'S' , // messageType: SystemEvent '\\xFF' , '\\xEE' , // uint16_t stock_locate: 65518 '\\xFF' , '\\xEE' , // uint16_t tracking_number: 65518 '\\xFF' , '\\xEE' , '\\xEE' , '\\xAA' , '\\x01' , '\\x02' , // TimeStamp time_stamp: 65518 'Q' // EventCode: MarketStart }; The job of the parser is to read these messages from a file (or a file descriptor like a socket) and to translate them into objects (classes in C++) that a program can understand. For instance, a C++ struct that represents an add order could look like this struct AddMessage { MessageType message_type ; uint16_t stock_locate ; uint16_t tracking_number ; TimeStamp time_stamp ; uint64_t order_ref_num ; BuySell buy_sell ; uint32_t shares ; char stock [ 8 ]; uint32_t price ; } Implementation There are many ways to write a message parser in C++. Searching on github shows several different ITCH messages parsers all taking somewhat different approaches. The one I liked the most and decided to base my implementation on is this one . It's actually a ITCH parser for the Australian Stock Exchange. I just adapted it to NASDAQ. At the core of the parser is a hierarchy of types and a switch statement handling each type. For the ITCH parser, this switch looks like the following: switch ( MessageType ( msg [ 0 ])) { case MessageType :: SystemEvent : return parseAs < SystemEvent > ( msg , len , std :: forward < Handler > ( handler )); case MessageType :: StockDirectory : return parseAs < StockDirectory > ( msg , len , std :: forward < Handler > ( handler )); case MessageType :: StockTradingAction : return parseAs < StockTradingAction > ( msg , len , std :: forward < Handler > ( handler )); ... default : return ParseStatus :: UnknownMessageType ; } The parseAs function converts the raw bytes into message structs. It does so using the following three lines in it's function body. MsgType msg { * reinterpret_cast < const MsgType *> ( buf )}; network_to_host ( msg ); handler ( msg ); It's worth looking at each one of these in detail. The first line MsgType msg { * reinterpret_cast < const MsgType *> ( buf )}; use reinterpret_cast to cast the bytes into the appropriate struct. This casting succeeds because the message structs are declared as packed in the message header file, e.g #pragma pack(push, 1) struct SystemEvent { MessageType message_type ; uint16_t stock_locate ; uint16_t tracking_number ; TimeStamp time_stamp ; EventCode event_code ; }; #pragma pack(pop) This means that the data members are aligned to match the message bytes, i.e. there is no byte padding between data members. For more on alignment and packing, see this stackoverflow thread. (https://stackoverflow.com/questions/3318410/pragma-pack-effect) There is a drawback to this approach. Namely, #pragma push is not supported by all C++ compilers. This limits the portability of the code. It's a fair point, but for the purpose of this exercise I choose to use #pragma push because it simplifies the casting from bytes to structs. Otherwise I would be forced to specify exactly how many bytes to read and in what order for each message type. There is another issue about portability I need to discuss. That's the byte swapping function network_to_host ( msg ); The incoming messages are in Big Endian (network) format. My system (host) is Little Endian (x86). The network_to_host functions simply reverse the byte order. There are OS specific utilities for doing this (e.g. in Linux), but that would again limit code portability (a common problem in C++!). For simplicity, I use the explicit byte swapping routines from the swap.hpp file such as For instance template <> inline void network_to_host ( uint16_t & x ) { x = ( x & 0x00FF ) << 8 | ( x & 0xFF00 ) >> 8 ; } template <> inline void network_to_host ( uint32_t & x ) { x = ( x & 0x000000FF ) << 24 | ( x & 0x0000FF00 ) << 8 | ( x & 0x00FF0000 ) >> 8 | ( x & 0xFF000000 ) >> 24 ; } Each message is then swapped by an appropriate function, for instance template <> inline void network_to_host < SystemEvent > ( SystemEvent & msg ) { network_to_host ( msg . message_type ); network_to_host ( msg . stock_locate ); network_to_host ( msg . tracking_number ); parse_ts ( msg . time_stamp ); network_to_host ( msg . event_code ); } Each struct field is mapped to the correct network_to_host function via C++'s template pattern matching functionality. The time stamp requires special treatment. We'll this topic discuss below. The final step in the parse function handler ( msg ); This is a generic method meant to redirect the messages to some destination. In this example it's a print function. It could in principle redirect to some other location such as a data base or an in-memory data structure representing a Limit Order Book. The Trouble with TimeStamps Keen readers will notice an annoying little wrinkle in the type hierarchy. Namely the TimeStamp field, which is present in all messages, is 6 bytes long. This is natural in the sense that it's enough bytes to cover the number of nanoseconds in a day. It's unnatural in the sense that there are no built-in 6 byte type in C++. To handle this situation we declare the TimeStamp to be a 6 bytes array struct TimeStamp { uint8_t data [ 6 ]; }; This ensures that when the message structs are initialized via the reinterpret_cast, the correct number of bytes are read from the file. However, when we swipe the bytes, we take the opportunity to upcast this 6 byte array an 8 byte int (uint64_t) via the following function uint64_t parse_ts ( char * a ){ return ( ( static_cast < uint64_t > ( bswap_16 ( * ( reinterpret_cast < uint16_t *> ( a )))) << 32 ) | static_cast < uint64_t > ( bswap_32 ( * ( reinterpret_cast < uint32_t *> ( a + 2 )))) ); } Life's easier with Python With the struct-casting, byte-swapping and printing functions in place, the parser is functional (barely). The next step ensure they work for all message types from the ITCH specification. Personally, I found this task tedious and error prone. Instead, I copied the spec into a yaml file and wrote a python program that parsers the yaml file and builds the structs, byte-swapping functions and printers in C++. This approach has the nice advantage that when the ITCH stand changes, the C++ parser can easily be built again from the specification file. An example entry from the yaml ITCH specification file looks like the following name : AddOrder type : Message MessageType : [ 1 , \"A\" , Add Order – No MPID Attribution Message. ] StockLocate : [ 2 , Integer , Locate code identifying the security ] TrackingNumber : [ 2 , Integer , Nasdaq internal tracking number ] Timestamp : [ 6 , Integer , Nanoseconds since midnight. ] OrderReferenceNumber : [ 8 , Integer , The unique reference number assigned to the new order at the time of receipt. ] BuySellIndicator : [ 1 , Alpha , The type of order being added. ] Shares : [ 4 , Integer , The total number of shares associated with the order being added to the book. ] Stock : [ 8 , Alpha , Stock symbol , right padded with spaces ] Price : [ 4 , Price , The display price of the new order. Refer to Data Types for field processing notes. ] --- name : BuySellIndicator type : Enum Buy : [ \"B\" , Buy Order. ] Sell : [ \"S\" , Sell Order. ] The python program consists of a series of yaml parsers, each one generating a different C++ file needed for the parser. As always, you can find code for this blog on my github page. See you next time.","tags":"Finance","url":"/parsing-itch-messages-in-c.html","loc":"/parsing-itch-messages-in-c.html"},{"title":"Local Volatility in PyQL","text":"Summary: Local Volatility Surface in PyQL Following on my previous post , I wanted to review the important concept of Local Volatility. Many books and articles 1 , 2 are dedicated to discussing this topic. I won't go into great detail here, I just want to give a basic overview along with implementation details in PyQL. Essentially, the idea of Local Volatility is to identify a level-dependent diffusion that exactly reproduces market implied volatilities. The contribution of Dupire was to prove that this diffusion is unique while also providing a convenient technique for deriving it from market quotes. The full derivation of the local volatility function is given in section 2.2 and the appendix of 2 . Here, I'll just give a sketch of the approach. To begin, consider a generic, level-dependent , 1-D Brownian motion $$ \\frac{\\partial S}{S} = \\mu(t, S)\\partial t + \\sigma(t,S)\\partial W $$ Note that, the volatility for this process is not itself stochastic. We can use this process to model the diffusion on a equity spot price in the presence of a short rate \\(r(t)\\) and a dividend yield \\(D(t)\\) . A European call option for this process then satisfies a modified version of the Black-Scholes equation: $$ \\frac{\\partial{C}}{\\partial{t}} = \\frac{\\sigma&#94;2K&#94;2}{2}\\frac{\\partial&#94;2C}{\\partial K&#94;2} + (r(t) - D(t))\\left(C- K\\frac{\\partial{C}}{\\partial{K}}\\right) $$ We can simplify this equation by writing \\(C(S_0, K,T)\\) as a function of the forward price \\(F_T=S_0\\exp(\\int_0&#94;T\\mu(t)dt) = S_0\\exp(\\int_0&#94;T(r(t) - D(t))dt)\\) , (i.e. use the forward measure) In these units, Black-Scholes equation simplifies to $$ \\frac{\\partial C}{\\partial t} = \\frac{\\sigma&#94;2K&#94;2}{2}\\frac{\\partial&#94;2 C}{\\partial K&#94;2} $$ or solving for the volatility gives Dupire's equation $$ \\sigma&#94;2(K,T) = \\frac{\\frac{\\partial C}{\\partial T}}{\\frac{1}{2}K&#94;2\\frac{\\partial&#94;2 C}{\\partial K&#94;2}} $$ Market quotes are usually given in terms of Black-Scholes implied volatilities. We can express the local volatility in terms of these quantities by equating the price equations for the two models $$ C_{local}(S_0, K,T) = C_{BS}(S_0, K, \\sigma_{BS}, T) $$ The strategy is then to solve for the local volatility in terms of the Black-Scholes impliedvolatility since we have a closed form expression for the \\(C_{BS}\\) . The full derivation is given in section 2.2 of 2 . Here we just reproduce the results. Namely, using more convenient units, the Black-Scholes total variance $$ w(S_0,K,T) = \\sigma&#94;2_{BS}(S_0,K,T)T $$ and log-moneyness $$y = \\ln\\left(\\frac{K}{F_T}\\right)$$ We can show that the local variance $$v_{local} = \\sigma&#94;2(S_0, K,T)$$ satisfies the following expression: $$ v_{local} = \\frac{\\frac{\\partial w}{\\partial T}}{1 - \\frac{y}{w}\\frac{\\partial w}{\\partial y} + \\frac{1}{4}\\left(-\\frac{1}{4} - \\frac{1}{w} + \\frac{y&#94;2}{w&#94;2}\\right)\\left(\\frac{\\partial w}{\\partial y}\\right)&#94;2 + \\frac{1}{2}\\left(\\frac{\\partial&#94;2 w}{\\partial y&#94;2}\\right)} $$ Quantlib implements this equation in ql.termstructure.volatility.equityfx.localvolsurface In the code excerpt below we, show the LocalVolSurface is used in PyQL. It follows a similiar interface and BlackVarianceSurface given in the previous post . calc_date = Date ( 6 , 11 , 2015 ) Settings () . evaluation_date = calc_date risk_free_rate = 0.01 dividend_rate = 0.0 day_count = Actual365Fixed () calendar = UnitedStates () flat_ts = FlatForward ( calc_date , risk_free_rate , day_count ) dividend_ts = FlatForward ( calc_date , dividend_rate , day_count ) expiration_dates = [ Date ( 6 , 12 , 2015 ), Date ( 6 , 1 , 2016 ), Date ( 6 , 2 , 2016 ), Date ( 6 , 3 , 2016 ), Date ( 6 , 4 , 2016 ) ] strikes = [ 527.50 , 560.46 , 593.43 , 626.40 ] data = np . array ( [ [ 0.37819 , 0.34450 , 0.37419 , 0.37498 , 0.35941 ], [ 0.34177 , 0.31769 , 0.35372 , 0.35847 , 0.34516 ], [ 0.30394 , 0.29330 , 0.33729 , 0.34475 , 0.33296 ], [ 0.27832 , 0.27614 , 0.32492 , 0.33399 , 0.32275 ] ] ) vols = Matrix . from_ndarray ( data ) black_var_surf = BlackVarianceSurface ( calc_date , calendar , expiration_dates , strikes , vols , day_count ) spot = 659.37 strike = 600.0 expiry = 0.2 # years local_vol_surf = LocalVolSurface ( black_var_surf , flat_ts , dividend_ts , spot ) print ( \"local vol: \" , local_vol_surf . localVol ( expiry , strike )) In the plots below, we can see what the LocalVolSurface looks like: Take a look at the PyQL bindings on my github to see an example of the LocalVolSurface . See you next time. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Finance","url":"/local-volatility-in-pyql.html","loc":"/local-volatility-in-pyql.html"},{"title":"Black Variance Surface in PyQL","text":"Summary: Building Black Variance Surfaces in PyQL In my previous post on Variance Swaps, I neglected to mention an important implementation detail. If you look the Variance Swap unittest or example script you'll see that replicating pricer requires a BlackVarianceSurface object. I had to build bindings for these in PyQL so I thought I'd mention how they work. BlackVarianceSurface is a volatility termstructure that implements different types of 2-d interpolation routines between implied volatility quotes. I included two types of interpolations from the Quantlib source code: Bilinear Bicubic To over simplify, Bilinear linearly interpolates between neighboring quotes. Bicubic uses a cubic spline routine (third order polynomial) to smoothly interpolate between points. See for wikipedia for a nice summary of the differences between bicubic vs. bilinear interpolation. The python interface is given in the example below ... dc = Actual365Fixed () calendar = UnitedStates () calculation_date = Date ( 6 , 11 , 2015 ) spot = 659.37 Settings . instance () . evaluation_date = calculation_date dividend_yield = SimpleQuote ( 0.0 ) risk_free_rate = 0.01 dividend_rate = 0.0 # bootstrap the yield/dividend/vol curves flat_term_structure = FlatForward ( reference_date = calculation_date , forward = risk_free_rate , daycounter = dc ) flat_dividend_ts = FlatForward ( reference_date = calculation_date , forward = dividend_yield , daycounter = dc ) dates = [ Date ( 6 , 12 , 2015 ), Date ( 6 , 1 , 2016 ), Date ( 6 , 2 , 2016 ), Date ( 6 , 3 , 2016 ), ] strikes = [ 527.50 , 560.46 , 593.43 , 626.40 ] data = np . array ( [ [ 0.37819 , 0.34177 , 0.30394 , 0.27832 ], [ 0.3445 , 0.31769 , 0.2933 , 0.27614 ], [ 0.37419 , 0.35372 , 0.33729 , 0.32492 ], [ 0.34912 , 0.34167 , 0.3355 , 0.32967 ], [ 0.34891 , 0.34154 , 0.33539 , 0.3297 ] ] ) vols = Matrix . from_ndarray ( data ) # Build the Black Variance Surface black_var_surf = BlackVarianceSurface ( calculation_date , NullCalendar (), dates , strikes , vols , dc ) strike = 600.0 expiry = 0.2 # years # The Surface interpolation routine can be set below (Bilinear is default) black_var_surf . set_interpolation ( Bilinear ) print ( \"black vol bilinear: \" , black_var_surf . blackVol ( expiry , strike )) black_var_surf . set_interpolation ( Bicubic ) print ( \"black vol bicubic: \" , black_var_surf . blackVol ( expiry , strike )) As a sanity checked, I re-implemented results from an excellent blog post by Gouthaman Balaraman. In that post, the author builds a BlackVarianceSurface object using Quantlib's swig bindings. The plots for the same market data are given below: For Bilinear interpolation, the surface looks like For Bicubic interpolation, the surface looks like As you can see from the plots, the bicubic is slightly smoother than bilinear. Take a look at the PyQL bindings on my github page to see how the BlackVarianceSurface is implemented. The script to generate the plots is here One thing that I clearly need to improve is the flexibility on the volatility data structure input to the BlackVarianceSurface constructor. It would be better to allow the vols data structure to be either a list of lists, a numpy array, or a QL Matrix object. I'll try to fix that at some point. See you next time.","tags":"Finance","url":"/black-variance-surface-in-pyql.html","loc":"/black-variance-surface-in-pyql.html"},{"title":"Variance Swaps in PyQL","text":"Summary: We review the Variance Swap replicating pricer in QuantLib and its implementation in PyQL Introduction Recently, I had the opportunity to extend the PyQL library to include variance swaps pricers. I thought I'd take the chance to review the pricing of Variance Swaps in Quantlib (to refresh my own memory if nothing else). As a reminder, a variance swap is a forward contract on realized variance, i.e. it has the following payoff at maturity $$ N(\\sigma_R&#94;2{\\tau} - K) $$ Where \\(N\\) is the notional, \\(\\sigma_R&#94;2{\\tau}\\) is the realized variance at maturity, \\(\\tau\\) , and \\(K\\) is the strike. This instrument can be used to provide pure exposure to variance. This is different then, for instance, a vanilla option which has variance (or volatility) exposure but also includes exposure to other risk factors such as the spot risk (delta). Quantlib includes two different pricing engines, ReplicatingVarianceSwapEngine and MCVarianceSwapEngine . As you might have guessed, ReplicatingVarianceSwapEngine uses a replicating portfolio to price a VarianceSwap and MCVarainceSwapEngine uses a Monte Carlo simulation. For this post, I'm going to focus on the replicating engine as the MCEngine is conventional and not terribly interesting. The replicating portfolio technique is described in thorough detail in Derman In essence, the idea of this (or any) replicating pricer is to reproduce the payoff of the variance swap using a portfolio of liquid vanilla instruments. In Derman , the authors show that a replicating portfolio can be constructed from a weighted combination of European Calls and Puts. We derive this portfolio in the next section. The derivation the the replicating portfolio is somewhat indirect. The authors introduce a fictitious instrument known as log contract that exactly replicates the variance swap payoff. They then show that the log contract can itself be replicated by the particular combination of European puts and calls. That being said, the Quantlib implementation is pretty straight forward. I directly adapted the variance swap unittests from Quantlib into PyQL ( tests/test_variance_swap.py ) You can find an example variance swap using the ReplicatingVarianceSwapEngine in that script. The important sections are given below # The Variance Swap is constructed #Type, Strike, Notional, Start, End strike = 0.04 notional = 50000 start = today () end = start + int ( 0.246575 * 365 + 0.5 ) # This is weird value but it was in the Quantlib unittest var_swap = VarianceSwap ( SwapType . Long , 0.04 , 50000 , start , end ) # Option Data used in the replicating engine replicating_option_data = [ { 'type' : OptionType . Put , 'strike' : 50 , 'v' : 0.30 }, { 'type' : OptionType . Put , 'strike' : 55 , 'v' : 0.29 }, { 'type' : OptionType . Put , 'strike' : 60 , 'v' : 0.28 }, { 'type' : OptionType . Put , 'strike' : 65 , 'v' : 0.27 }, { 'type' : OptionType . Put , 'strike' : 70 , 'v' : 0.26 }, { 'type' : OptionType . Put , 'strike' : 75 , 'v' : 0.25 }, { 'type' : OptionType . Put , 'strike' : 80 , 'v' : 0.24 }, { 'type' : OptionType . Put , 'strike' : 85 , 'v' : 0.23 }, { 'type' : OptionType . Put , 'strike' : 90 , 'v' : 0.22 }, { 'type' : OptionType . Put , 'strike' : 95 , 'v' : 0.21 }, { 'type' : OptionType . Put , 'strike' : 100 , 'v' : 0.20 }, { 'type' : OptionType . Call , 'strike' : 100 , 'v' : 0.20 }, { 'type' : OptionType . Call , 'strike' : 105 , 'v' : 0.19 }, { 'type' : OptionType . Call , 'strike' : 110 , 'v' : 0.18 }, { 'type' : OptionType . Call , 'strike' : 115 , 'v' : 0.17 }, { 'type' : OptionType . Call , 'strike' : 120 , 'v' : 0.16 }, { 'type' : OptionType . Call , 'strike' : 125 , 'v' : 0.15 }, { 'type' : OptionType . Call , 'strike' : 130 , 'v' : 0.14 }, { 'type' : OptionType . Call , 'strike' : 135 , 'v' : 0.13 }, ] # The engine is constructed engine = ReplicatingVarianceSwapEngine ( process , call_strikes , put_strikes , 5.0 ) # dK, shift below lowest put strike # attach the engine to the swap var_swap . set_pricing_engine ( engine ) print ( \"strike: \" , var_swap . strike ) print ( \"postion: \" , var_swap . position ) print ( \"variance: \" , var_swap . variance ) The output is: strike : 0.04 postion : SwapType . Long variance : 0.0419 Deriving the Replicating Portfolio Briefly, from the definition of the variance strike given above, we see that the par strike of a variance swap is the expected realized variance, i.e. $$ K_{var} = \\frac{1}{T}\\mathbf{E}\\left[\\int&#94;T_0 \\sigma&#94;2(t, \\dots)dt\\right] $$ The first step in the derivation is to re-write this expression. We consider a generic Ito process of the following form: $$\\frac{dS_t}{S_t} = \\mu(t, \\dots) dt + \\sigma(t, \\dots) dW_t $$ Where \\(\\mu\\) and \\(\\sigma\\) can be time or level dependent. Applying Ito's lemma to \\(\\ln(S_t)\\) and subtracting the above equation gives $$\\frac{dS_t}{S_t} - d\\left(\\ln(S_t)\\right) = \\frac{1}{2}\\sigma&#94;2dt$$ We insert this expression into the \\(K_{var}\\) definition to get $$ \\begin{align} K_{var} =& \\frac{2}{T}\\mathbf{E}\\left[\\int&#94;T_0 \\frac{\\partial S_t}{S_t} - \\ln\\left(\\frac{S_T}{S_0}\\right)\\right] \\\\ \\end{align} $$ Next, we partition the strike domain by introducing a cutoff strike value, \\(S_* \\in (0, \\infty)\\) . In what follows this cutoff will be set to lower bound of put strikes, but for now we leave it arbitrary. The allows us the write \\(K_{var}\\) as $$ \\begin{align} K_{var} =& \\frac{2}{T}\\mathbf{E}\\left[\\int&#94;T_0 \\frac{\\partial S_t}{S_t} - \\frac{S_T- S_*}{S_*}- \\ln\\left(\\frac{S_*}{S_0}\\right) + \\frac{S_T- S_*}{S_*} -\\ln\\left(\\frac{S_T}{S_*}\\right)\\right] \\end{align} $$ We can then use the fact that in the risk neutral measure $$ \\mathbf{E}\\left[\\int&#94;T_0 \\frac{\\partial S_t}{S_t}dt\\right] = rT $$ (i.e. martingales are driftless) Distributing the expectation value gives $$ K_{var} = \\frac{2}{T}\\left[rT - \\left(\\frac{S_0}{S_*}e&#94;{rT} - 1\\right) - \\ln\\left(\\frac{S_*}{S_0}\\right)\\right] + e&#94;{rT}\\frac{2}{T}\\mathbf{E}\\left[ \\frac{S_T- S_*}{S_*} -\\ln\\left(\\frac{S_T}{S_*}\\right)\\right] $$ This expression implies that the variance swap can be replicated by an option with the following payoff $$ f(S_T) = \\frac{2}{T}\\left(\\frac{S_T-S_*}{S_*} - \\ln\\frac{S_T}{S_*}\\right) \\label{eq1} $$ This option is the so-called log-contract , which obviously only exists in the minds of quants. The final trick to recognize that the log-contract can itself be replicated as linear combination of European calls and puts (which thankfully do exist!). We first consider the strike, \\(K\\) , as a continuous variable. We then can build a portfolio that matches the log contract's payoff by weighting the options with the inverse of their strikes. Namely, we can show that $$ \\frac{S_T- S_*}{S_*} -\\ln\\left(\\frac{S_T}{S_*}\\right) = \\int&#94;{S_*}_0 dK\\frac{\\max[K- S(T),0]}{K&#94;2} + \\int_{S_*}&#94;{\\infty} dK\\frac{\\max[S(T)- K, 0]}{K&#94;2} $$ So we can approximate the price of the log-contract by expanding the expectation value as a sum of European calls and puts $$ \\begin{align} \\mathbf{E}\\left[ \\frac{S_T- S_*}{S_*} -\\ln\\left(\\frac{S_T}{S_*}\\right)\\right] &= \\mathbf{E}\\left[ \\int&#94;{S_*}_0 dK\\frac{\\max[K- S(T),0]}{K&#94;2} + \\int_{S_*}&#94;{\\infty} dK\\frac{\\max[S(T)- K, 0]}{K&#94;2} \\right] \\\\ & =\\int&#94;{S_*}_0 \\frac{dK}{K&#94;2} \\mathbf{E}\\left[\\max[K- S(T),0]\\right] + \\int_{S_*}&#94;{\\infty} \\frac{dK}{K&#94;2}\\mathbf{E}\\left[\\max[S(T)- K, 0]\\right] \\\\ & =\\int&#94;{S_*}_0 \\frac{dK}{K&#94;2} P(K,T) + \\int_{S_*}&#94;{\\infty} \\frac{dK}{K&#94;2}C(K,T) \\\\ \\end{align} $$ \\(S_*\\) is then set to the put strike lower bound, \\(S_* = K_{Put_1} - dK\\) . In the numerical example given above \\(S_* = 50 - 5 = 45\\) . Take a look at the PyQL bindings on my github page to see how the ReplicatingVarianceSwapEngine is implemented. See you next time. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Finance","url":"/variance-swaps-in-pyql.html","loc":"/variance-swaps-in-pyql.html"},{"title":"Simulating the Heston Process","text":"Summary: We investigate different discretization schemes for the Heston Process, looking at issues of numerical bias and ease of implementation In today's post, I will examine a few issues related to numerical simulation of the Heston model. As a quick reminder, the Heston process is a stochastic volatility model defined by the following sde: $$ dS_t = (r-d) S_t dt + \\sqrt{V_t} S_t dW&#94;s_t $$ $$ dV_t = \\kappa (\\theta - V_t) dt + \\sigma \\sqrt{V_t} dW&#94;\\upsilon_t $$ $$ dW&#94;s_t dW&#94;\\upsilon_t = \\rho dt $$ for asset price \\(S_t\\) . \\(dW_t&#94;s\\) and \\(dW_t&#94;{\\nu}\\) are Wiener processes with correlation \\(\\rho\\) . As usual, \\(r\\) and \\(d\\) are the risk free and dividend rates respectively. The Heston model is an excellent benchmarking model due to the suprising fact that that it's probability distribution function (pdf) has a closed formed expression. It's a one of five such processes in 1 or 2 dimensions PHL . With the closed form pdf, we can easily compare the performance of the Heston model's numerical simulation by comparing PV's of a Monte Carlo pricer with an analytic pricer. To carry out a numerical simulation we first need to choose a discretization scheme. That is, we need to approximate the infinitesimal diffusion process with a finite scheme to be used in the simulation code. Any such scheme may potentially introduce bias into the simulation that could alter the realized moments of the pdf. The simplest possible scheme is a first order expansion, i.e. an Euler Mayorana (or just ''Euler'') scheme. Transforming to \\(\\ln S\\) and discretizing \\(t\\) to step size \\(\\Delta\\) gives $$ \\ln S(t + \\Delta) = \\ln S(t) + \\left(r - d - \\frac{1}{2}V(t)\\right)\\Delta +\\sqrt{V(t)} Z_S\\sqrt{\\Delta} $$ $$ V(t + \\Delta) = V(t) + \\kappa\\left(\\theta - V(t)\\right)\\Delta + \\sigma \\sqrt{V(t)} Z_V \\sqrt{\\Delta} $$ However, it turns out that this scheme does introduce bias. Furthermore, in this scheme it's possible to drive the variance process to negative values, which is outside the range of the pdf and causes the simulation to stop as \\(\\sqrt{V}\\) becomes imaginary. One immediate fix to this problem is the floor V at 0, giving the so called \"Truncated\" scheme. Other schemes are based on higher order expansions of the sde or utilizing the closed form expression for the pdf. These are documented extensively in the literature Anderson . We list some of the schemes below Euler Mayorana Truncated Quadratic Exponential Broadie Kaya (Exact Simulation) Broadie Kaya is based off direct sampling from the pdf but is rather difficult to implement numerically. The Quadratic Exponential (developed in Anderson ) approximates the pdf using simplified functions and moment matching. The pdf for high and low values of the variance are matched against two different functions and threshold is implemented to switch between the two functions during a diffusion. This technique allows one to minimize variance while also easing the numerical implementation. As such, this scheme it is often the preferred scheme for Heston simulations. QuantLib Implementation QuantLib includes several of these discretization schemes in their Heston Process class. Let's see what a simulation looks like. Quick note: Although the HestonProcess class is available through the SWIG bindings, it does not expose the discretizations argument. Instead, we use PyQL , which has all the discretizations available. from quantlib.processes.heston_process import * from quantlib.quotes import SimpleQuote from quantlib.settings import Settings from quantlib.termstructures.yields.flat_forward import FlatForward from quantlib.time.api import today , TARGET , ActualActual , Date , Period , Years from quantlib.models.equity.heston_model import ( HestonModel , HestonModelHelper ) from quantlib.pricingengines.api import ( AnalyticHestonEngine ) from quantlib.pricingengines.vanilla.mceuropeanhestonengine import MCEuropeanHestonEngine from quantlib.instruments.api import ( PlainVanillaPayoff , EuropeanExercise , VanillaOption , EuropeanOption ) Defining the Heston Process def flat_rate ( forward , daycounter ): return FlatForward ( forward = forward , settlement_days = 0 , calendar = TARGET (), daycounter = daycounter ) settings = Settings . instance () settlement_date = today () settings . evaluation_date = settlement_date day_counter = ActualActual () interest_rate = 0.7 dividend_yield = 0.4 risk_free_ts = flat_rate ( interest_rate , day_counter ) dividend_ts = flat_rate ( dividend_yield , day_counter ) maturity = Period ( 10 , Years ) exercise_date = settlement_date + maturity # spot s0 = SimpleQuote ( 100.0 ) # Available descritizations #PARTIALTRUNCATION #FULLTRUNCATION #REFLECTION #NONCENTRALCHISQUAREVARIANCE #QUADRATICEXPONENTIAL #QUADRATICEXPONENTIALMARTINGALE #BROADIEKAYAEXACTSCHEMELOBATTO #BROADIEKAYAEXACTSCHEMELAGUERRE #BROADIEKAYAEXACTSCHEMETRAPEZOIDAL # Heston Model params v0 = 0.05 kappa = 5.0 theta = 0.05 sigma = 1.0e-4 rho = - 0.5 def gen_process ( desc ): process = HestonProcess ( risk_free_ts , dividend_ts , s0 , v0 , kappa , theta , sigma , rho , desc ) return process processes = { \"REFLECTION\" : gen_process ( REFLECTION ), \"PARTIALTRUNCATION\" : gen_process ( PARTIALTRUNCATION ), \"QUADRATICEXPONENTIAL\" : gen_process ( QUADRATICEXPONENTIAL ), \"QUADRATICEXPONENTIALMARTINGALE\" : gen_process ( QUADRATICEXPONENTIALMARTINGALE ), } Visualizing the Simulation Note: The simulate function is not part of Quantlib. It has been added to the PyQL interface (see folder quantlib/sim ). from quantlib.sim.simulate import simulate_process from quantlib.time_grid import TimeGrid import pandas as pd import numpy as np import matplotlib.pyplot as plt % matplotlib inline import seaborn as sns sns . set ( style = \"white\" , rc = { \"axes.facecolor\" : ( 0 , 0 , 0 , 0 )}) # simulate and plot Heston paths paths = 200 steps = 100 horizon = 2 seed = 154 grid = TimeGrid ( horizon , steps ) fig , axs = plt . subplots ( figsize = ( 14 , 12 ), nrows = 2 , ncols = 2 ) flat_axs = axs . reshape ( - 1 ) for i , key in enumerate ( processes . keys ()): flat_axs [ i ] . plot ( list ( grid ), simulate_process ( processes [ key ], paths , grid , seed )) flat_axs [ i ] . set_xlabel ( 'Time' ) flat_axs [ i ] . set_ylabel ( 'Stock Price' ) flat_axs [ i ] . set_title ( ' %s ' % key ) Let's look at of the diffusions in detail, for example the evolution of the Heston process using Partial Truncation looks like this res = simulate_process ( processes [ \"PARTIALTRUNCATION\" ], paths , grid , seed ) x = res [[ 25 , 50 , 75 , 100 ],:] . reshape ( 4 * paths ) g = np . repeat ([ \"T=0.5\" , \"T=1.0\" , \"T=1.5\" , \"T=2.0\" ], paths ) df = pd . DataFrame ( dict ( x = x , g = g )) # Initialize the FacetGrid object pal = sns . cubehelix_palette ( 10 , rot =-. 25 , light =. 7 ) g = sns . FacetGrid ( df , row = \"g\" , hue = \"g\" , aspect = 10 , size = 1.6 , palette = pal ) g . map ( sns . distplot , \"x\" , bins = 40 , kde = True , rug = False ) g . map ( plt . axhline , y = 0 , lw = 2 , clip_on = False ) def label ( x , color , label ): ax = plt . gca () ax . text ( 0 , . 3 , label , fontweight = \"bold\" , color = color , ha = \"left\" , va = \"center\" , transform = ax . transAxes ) g . map ( label , \"x\" ) g . fig . subplots_adjust ( hspace =. 25 ) g . set_titles ( \"\" ) g . set ( yticks = []) g . despine ( bottom = True , left = True ) Comparing the PV's of different engines To examine the performance of the numerical schemes, we simply compare the PV's the same Call Option priced with an analytic and Monte Carlo engine. The results for different time steps and schemes are given in the table below # Build a Call Option payoff = PlainVanillaPayoff ( 'Call' , 105 ) exercise = EuropeanExercise ( exercise_date ) option = EuropeanOption ( payoff , exercise ) results = { \"REFLECTION\" : [], \"PARTIALTRUNCATION\" : [], \"QUADRATICEXPONENTIAL\" : [], \"QUADRATICEXPONENTIALMARTINGALE\" : [], } # steps per year steps = [ 1 , 4 , 8 , 16 ] for k , v in processes . items (): for step in steps : mc_engine = MCEuropeanHestonEngine ( v , steps_per_year = step , required_samples = 500 , seed = 1234 , ) option . set_pricing_engine ( mc_engine ) results [ k ] . append (( np . around ( option . npv , 5 ), np . around ( option . error_estimate , 5 ))) results [ \"time steps per year\" ] = steps results_df = pd . DataFrame ( data = results ) cols = results_df . columns . tolist () cols = cols [ - 1 :] + cols [ - 2 : - 1 ] + cols [: - 2 ] results_df = results_df [ cols ] results_df time steps per year REFLECTION PARTIAL TRUNCATION QUADRATIC EXPONENTIAL QUADRATIC EXPONENTIAL MARTINGALE 1 (1.64404, 0.05392) (1.74109, 0.03607) (1.71144, 0.02744) (1.71527, 0.02811) 4 (1.64447, 0.06608) (1.74109, 0.03607) (1.71144, 0.02744) (1.71527, 0.02811) 8 (1.98966, 0.04768) (1.77399, 0.03692) (1.70905, 0.02727) (1.73049, 0.02829) 16 (1.80227, 0.03563) (1.75864, 0.0351) (1.70581, 0.02687) (1.72956, 0.02818) Conclusion As we saw, there are many ways to simulate the Heston model, each with strengths and weaknesses with regards to bias/accuracy and runtime performance. In general, the Quadratic Exponential scheme performed better than Euler-type schemes, especially for simulations with fewer time steps per year. In a future post, I will visit the local stochastic volatility version of the Heston model to see how it's simulation works. Is there a better way? If you take a look at the HestonProcess class file in QuantLib you will see that it, for each discretization, there is a different diffusion implementation. The user chooses the discretization from an enumerated list. The downside to this approach is that , in addition to creating bloated code, the list is always finite. What happens when someone wants to create a new discretization scheme that's not on the list? She has to add the diffusion implementation directly into the HestonProcess class. A better approach might be to allow the user to build her own discretization in Python and then \"compile-down\" to C++ in the similar way to how the finite difference stencils are created in Devito [see my earlier post] This may just be hopeful speculation on my part :). Until next time. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Finance","url":"/simulating-the-heston-process.html","loc":"/simulating-the-heston-process.html"},{"title":"Devito and Option Pricing","text":"Summary: Devito allows users to build finite difference schemes in python and \"compile-down\" to optimized C++ Finite difference methods have a long history in quantitative finance. They are the preferred technique to price and risk manage instruments based on low dimensional probabilistic models that lack analytic solutions. To implement the finite difference method, one chooses a discretization scheme (or stencil) that approximates the derivatives in the model. This scheme converts a system of differential equations into a system of algebraic equations that can be solved numerically. Naturally, a scheme introduces error into the calculation that is roughly proportional to the order of the discretization. Higher order schemes decrease the error in the system but are more difficult to implement by hand in compiled numerical code. Most option pricing libraries provide utilities for building common, low-order schemes. For instance, QuantLib provides forward, backward and central difference operators that can be combined to build a finite difference pricing engine for a given model. A new python library called Devito takes a different approach to finite difference model building. Instead of building a discretization scheme directly in C++, it instead allows users to build an arbitrary scheme in python and \"compile\" that scheme down to optimized C++. This code can either be JIT compiled and executed immediately or retained for later execution. In order to make this approach possible, Devito leverages Sympy, the python-based computer algebra system. Devito uses Sympy to algebraically manipulate the system of differential equations and generates the equivalent stencil equations used in the numerical implementation. It then generates optimized C++ code after passing the stencil equations through various optimization algorithms. This approach is equivalent to what happens in a compiler. To wit, \"high-level\" code is parsed, passed through optimization algorithms and printed down to \"low-level\" code. In this example, python is the high level code and C++ is the low level code. This approach has several advantages; one, the python implementation is close to the mathematical language; two, the often tedious algebraic manipulations needed to build the stencil is off-loaded to the computer algebra system (Sympy); three, the C++ implementation can be highly optimized of efficient computation. Black Scholes Equation As a first example, let's implement a finite difference solution of the Black Scholes model, the Hello World of quant finance. The Black Scholes PDE for a European option with strike, K, spot, S, volatility \\(\\sigma\\) , risk free rate, r, expiring at T, is given by $$ \\frac{\\partial V}{\\partial t} + \\frac{\\sigma&#94;2S&#94;2}{2}\\frac{\\partial &#94;2 V}{\\partial S&#94;2} + rS\\frac{\\partial V}{\\partial S} - rV = 0 $$ with the terminal condition for a Call option given by $$V(S,T) = \\max(S-K,0)$$ This PDE can be solved analytically: $$V(S,K,t,r, \\sigma)=SN(d_1)−Ke&#94;{−rt}N(d_2)$$ $$d_1=\\frac{\\ln\\left(\\frac{S}{K}\\right)+\\left(r+0.5\\sigma&#94;2\\right)T}{\\sigma\\sqrt{T}}\\,\\,\\,d_2=d_1−\\sigma\\sqrt{T}$$ In Numpy, this pricing function is import numpy as np from scipy.stats import norm import matplotlib.pyplot as plt % matplotlib inline #Black Scholes Formula def d1 ( S0 , K , r , sigma , t ): return ( np . log ( S0 / K ) + ( r + sigma ** 2 / 2 ) * t ) / ( sigma * np . sqrt ( t )) def d2 ( S0 , K , r , sigma , t ): return ( np . log ( S0 / K ) + ( r - sigma ** 2 / 2 ) * t ) / ( sigma * np . sqrt ( t )) def BlackScholes ( S0 , K , r , sigma , T ): return S0 * norm . cdf ( d1 ( S0 , K , r , sigma , T )) - K * np . exp ( - r * T ) * norm . cdf ( d2 ( S0 , K , r , sigma , T )) BS Finite Difference Scheme in Numpy In order to solve the BS PDE using finite difference techniques, we need to choose a discretization scheme. Keeping things simple, we choose central differences in space and forward differencing in time. Indexing the spatial dimension with \\(i\\) and the temporal dimension with \\(n\\) , Black Scholes PDE then becomes $$V_{i}&#94;{n+1} = V_{i}&#94;n - \\frac{\\sigma&#94;2 (i\\Delta S)&#94;2 \\Delta t}{2\\Delta S&#94;2}\\left(V_{i+1}&#94;n - 2 V_{i}&#94;n + V_{i-1}&#94;n\\right) - r(i\\Delta S)\\frac{\\sigma \\Delta t}{2\\Delta S}\\left(V_{i+1}&#94;n- V_{i-1}&#94;n \\right) + r \\Delta t V_{i}&#94;n$$ $$ = V_{i}&#94;n - \\frac{\\sigma&#94;2 (i)&#94;2 \\Delta t}{2}\\left(V_{i+1}&#94;n - 2 V_{i}&#94;n + V_{i-1}&#94;n\\right) -r(i)\\frac{\\sigma\\Delta t}{2}\\left(V_{i+1}&#94;n- V_{i-1}&#94;n \\right)+ r \\Delta t V_{i}&#94;n $$ Let's first implement a Numpy version of the scheme for comparison's sake. The numerical algorithm essentially works as follows Evaluate the pricing function at the boundary (payoff) Iterate backwards in time, solving the difference equation at each iteration Terminate at t=0 (the evaluation date) to obtain the present value (PV) In terms of data structures, all we need in order to implement this algorithm is two Numpy array buffers. A word on spatial boundary conditions We need to choose spatial boundary conditions on pricing function. In practice, this means enforcing some conditions on either the pricing function itself or derivatives of the pricing function. Following Wilmott Vol 3 , we choose the following boundary conditions For \\(S = 0 \\;\\; \\frac{\\partial V}{\\partial t}(0,t) - rV(0,t) = 0\\) In discrete form this becomes: $$V_n&#94;0 = (1 - r \\delta t) V_{k-1}&#94;0$$ For \\(S = S_{max}\\;\\; \\frac{\\partial&#94;2 V}{\\partial S&#94;2}(S,t) = 0\\) In discrete form this becomes: $$V_n&#94;i = 2V_n&#94;{i-1} - V_n&#94;{i-2}$$ In principle, one can choose different boundary conditions without greatly effecting the computed pricing functions. For the remaining variables needed to solve the problem we make the following choices # Some variable declarations needed for numerical implementation nx = 20 # number of space steps s = 0.2 # vol r = 0.05 # interest rate T = 1 # time to expiry K = 100 # Strike dt = ( 0.9 / ( s * nx ) ** 2 ) # time step size nt = int ( T / dt ) + 1 # number of time steps dt = T / nt dx = 2 * K / nx Defining a Call payoff function and initializing it at the t=T boundary def payoff ( S , K ): return np . maximum ( S - K , 0 ) v_payoff = np . vectorize ( payoff ) v_bs = np . vectorize ( BlackScholes ) # Init C at the payoff S = np . arange ( 0 ,( nx + 1 ) * dx , dx ) C = v_payoff ( S , K ) Now we can define a diffuse function that carries out the finite difference algorithm. Here we name our pricing function C for Call in order to distinguish it from V used below in the Devito implementation. def diffuse ( C , nt ): for n in range ( nt ): Cn = C . copy () delta = ( 0.5 / dx ) * ( Cn [ 2 :] - Cn [ 0 : - 2 ]) gamma = ( 1 / ( dx ** 2 )) * ( Cn [ 2 :] - 2 * Cn [ 1 : - 1 ] + Cn [ 0 : - 2 ]) theta = - ( 0.5 * s ** 2 ) * np . multiply ( np . square ( S [ 1 : - 1 ]), gamma ) - r * np . multiply ( S [ 1 : - 1 ], delta ) + r * Cn [ 1 : - 1 ] C [ 1 : - 1 ] = Cn [ 1 : - 1 ] - dt * theta #spatial bc's C [ 0 ] = Cn [ 0 ] * ( 1 - r * dt ) C [ nx - 1 ] = 2 * C [ nx - 2 ] - C [ nx - 3 ] With our implementation in place, let's diffuse the model back 10 time steps. diffuse ( C , nt = 10 ) Comparing this to analytic solution to BS gives C array([ 0.00000000e+00, 6.77083365e-15, 2.47797465e-11, 1.38986768e-08, 2.31379196e-06, 1.50919507e-04, 4.48551091e-03, 6.67669285e-02, 5.33585468e-01, 2.44970519e+00, 7.05423863e+00, 1.43368053e+01, 2.32467881e+01, 3.28816476e+01, 4.27765661e+01, 5.27502671e+01, 6.27445527e+01, 7.27434913e+01, 8.27433082e+01, 9.27431251e+01, 1.00000000e+02]) v_bs ( S0 = S , K = 100 , r = r , sigma = s , T = 10 * dt ) array([ 0.00000000e+00, 2.13184792e-53, 8.12368101e-27, 1.49012572e-15, 1.86102962e-09, 8.60357129e-06, 1.81033147e-03, 5.89084888e-02, 5.84232552e-01, 2.66791893e+00, 7.33121846e+00, 1.45076224e+01, 2.33168378e+01, 3.29033374e+01, 4.27809370e+01, 5.27490737e+01, 6.27415850e+01, 7.27399614e+01, 8.27396309e+01, 9.27395669e+01, 1.02739555e+02]) Plotting the PV gives reasonable looking results fig , ax = plt . subplots () ax . plot ( S , C ) Devito Implementation Now we want to implement the same model in Devito. First, a brief word about the Devito API. (For more info, consult the Devito documentation) A key idea in Devito is to present users with a Function object that has dual identity, one symbolic and the other numeric. That is, Function objects can be manipulated by Sympy in order to generate the stencil equations. Later, when the finite difference scheme is being numerically solved, Function objects hold numerical data in two Numpy array buffers in similar way to how data was stored during the Numpy implementation given above. This dual approach allows users to implement and solve the finite difference problems in a natural way. They don't have to implement and reason about separate data structures for the differential equations as well as the numerical implementation. For our implementation, we first need to declare instances of Grid , Function and TimeFunction (time varying function). We then define the stencil equation using the TimeFunction V 's Sympy identity. from devito import Grid , TimeFunction , Function , Operator from sympy import Eq , solve # Initialize `u` for space order 2 grid = Grid ( shape = ( nx + 1 ,), extent = ( 200. ,)) V = TimeFunction ( name = 'V' , grid = grid , space_order = 2 ) X = Function ( name = 'X' , grid = grid ) # Create an equation with second-order derivatives eq = Eq ( V . dt , 0.5 * s ** 2 * X * X * ( V . dx2 ) + r * X * ( V . dx ) - r * V ) stencil = solve ( eq , V . forward )[ 0 ] eq_stencil = Eq ( V . forward , stencil ) Looking at the stencil, one can see that it is equivalent to the Numpy version of the stencil defined earlier. print ( stencil ) (0.025*dt*h_x*(-V(t, x - h_x) + V(t, x + h_x))*X(x) + 0.02*dt*(-2.0*V(t, x) + V(t, x - h_x) + V(t, x + h_x))*X(x)**2 + 0.05*h_x**2*(-dt + 20.0)*V(t, x))/h_x**2 Next we implement boundary conditions by setting the Data attribute of V equal to the payoff value. We also enforce spatial boundary conditions as before. Finally, we construct an Operator object from the stencil and boundary conditions. This is the object that carries out the diffusion in order to solve the finite difference problem. # Init C at the payoff S = np . arange ( 0 ,( nx + 1 ) * dx , dx ) V . data [ 0 ] = v_payoff ( S , K ) V . data [ 1 ] = v_payoff ( S , K ) X . data [:] = S # Create boundary condition expressions x = grid . dimensions t = grid . stepping_dim bc = [ Eq ( V . indexed [ t + 1 , 0 ], V . indexed [ t , 0 ] * ( 1 - r * dt ))] # bottom bc += [ Eq ( V . indexed [ t + 1 , - 1 ], 2 * V . indexed [ t + 1 , - 2 ] - V . indexed [ t + 1 , - 3 ])] # top # Define the operator op = Operator ([ eq_stencil ] + bc ) The data arrays before simulation are initialized to the payoff boundary V . data [ 1 ] array([ 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 10., 20., 30., 40., 50., 60., 70., 80., 90., 100.], dtype=float32) Diffusing the operator 10 time steps as before we get op ( time = 10 , dt = dt ) CustomCompiler : compiled /tmp/devito-oebkv76j/ 41 acee72a2d7a564a563c14bcd4b5358a3ed3147 . c [ 1.23 s ] ========================================================================================= Section section_0 < 10 , 20 > with OI = 1.44 computed in 0.000 s [ 4.00 GFlops / s ] Section main < 10 > with OI = 1.67 computed in 0.000 s [ 0.11 GFlops /s, 0.01 GPts/s ] ========================================================================================= Now let's inspect the data array V . data [ 0 ] array([ 0.00000000e+00, 6.77083621e-15, 2.47797494e-11, 1.38986778e-08, 2.31379227e-06, 1.50919528e-04, 4.48551122e-03, 6.67669326e-02, 5.33585429e-01, 2.44970512e+00, 7.05423832e+00, 1.43368015e+01, 2.32467213e+01, 3.28807259e+01, 4.27697105e+01, 5.27169189e+01, 6.26308594e+01, 7.24206314e+01, 8.20446320e+01, 9.12274399e+01, 1.00405121e+02], dtype=float32) Comparing that with the Numpy implementation gives similar results C array([ 0.00000000e+00, 6.77083365e-15, 2.47797465e-11, 1.38986768e-08, 2.31379196e-06, 1.50919507e-04, 4.48551091e-03, 6.67669285e-02, 5.33585468e-01, 2.44970519e+00, 7.05423863e+00, 1.43368053e+01, 2.32467881e+01, 3.28816476e+01, 4.27765661e+01, 5.27502671e+01, 6.27445527e+01, 7.27434913e+01, 8.27433082e+01, 9.27431251e+01, 1.00000000e+02]) As one can see from the results, the closest agreement between all three solutions is away from the boundaries and the cusp S = K. This makes sense as the pricing function isn't smooth at those points. I will explore the numerical errors in a later post. For fun, let's you can take a look at the C++ code that was generated and JIT compiled during evaluation print ( op . ccode ) #define _POSIX_C_SOURCE 200809L #include \"stdlib.h\" #include \"math.h\" #include \"sys/time.h\" #include \"xmmintrin.h\" #include \"pmmintrin.h\" struct profile { double section_0 ; double section_1 ; } ; int Kernel ( float * restrict V_vec , float * restrict X_vec , const float dt , const float h_x , const int t_size , const int t_s , const int t_e , const int time_size , const int time_s , const int time_e , const int x_size , const int x_s , const int x_e , void * _timings ) { float ( * restrict V )[ x_size ] __attribute__ (( aligned ( 64 ))) = ( float ( * )[ x_size ]) V_vec ; float ( * restrict X ) __attribute__ (( aligned ( 64 ))) = ( float ( * )) X_vec ; struct profile * timings = ( struct profile * ) _timings ; /* Flush denormal numbers to zero in hardware */ _MM_SET_DENORMALS_ZERO_MODE ( _MM_DENORMALS_ZERO_ON ); _MM_SET_FLUSH_ZERO_MODE ( _MM_FLUSH_ZERO_ON ); struct timeval start_section_1 , end_section_1 ; gettimeofday ( & start_section_1 , NULL ); for ( int time = t_s , t0 = ( time ) % ( 2 ), t1 = ( time + 1 ) % ( 2 ); time < t_e - 1 ; time += 1 , t0 = ( time ) % ( 2 ), t1 = ( time + 1 ) % ( 2 )) { for ( int x = x_s + 1 ; x < x_e - 1 ; x += 1 ) { V [ t1 ][ x ] = ( 2.5e-2 F * dt * h_x * ( - V [ t0 ][ x - 1 ] + V [ t0 ][ x + 1 ]) * X [ x ] + 2.0e-2 F * dt * ( X [ x ] * X [ x ]) * ( - 2.0F * V [ t0 ][ x ] + V [ t0 ][ x - 1 ] + V [ t0 ][ x + 1 ]) + 5.0e-2 F * ( h_x * h_x ) * ( - dt + 2.0e+1 F ) * V [ t0 ][ x ]) / pow ( h_x , 2 ); } V [ t1 ][ 0 ] = 9.97222222222222e-1 F * V [ t0 ][ 0 ]; V [ t1 ][ - 1 ] = - V [ t1 ][ - 3 ] + 2 * V [ t1 ][ - 2 ]; } gettimeofday ( & end_section_1 , NULL ); timings -> section_1 += ( double )( end_section_1 . tv_sec - start_section_1 . tv_sec ) + ( double )( end_section_1 . tv_usec - start_section_1 . tv_usec ) / 1000000 ; return 0 ; } Conclusion In this post we solved Black Scholes equation using finite difference methods in both Numpy and Devito. As you can see, their output is similar but their implementation is different. The great thing about Devito is that it allows users to build complex stencils and solve pde's without having to worry about hand writing optimized C++ code. This achieves a nice seperation of concerns between model building in Python and numerical implemenation in C++. In future posts, I plan on implementing more finance models in Devito and comparing Devito's performance with conventional pricing libraries like QuantLib. Until then, thanks for visiting my blog. See you next time. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Finance","url":"/devito-and-option-pricing.html","loc":"/devito-and-option-pricing.html"}]};